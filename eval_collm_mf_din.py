#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CoRAæ¨¡å‹åˆ†ç±»åˆ«è¯„ä¼°è„šæœ¬ - åŸºäºè®­ç»ƒè„šæœ¬train_collm_mf_din.py
ä¸“æ³¨äºæ¨èä»»åŠ¡è¯„ä¼°ï¼Œæ”¯æŒæŒ‰instructionåˆ†ç±»åˆ«è¯„ä¼°å’Œæ¶ˆèå®éªŒ
å›¾åƒç”Ÿæˆæ¨¡å—å·²ç¦ç”¨

æ”¯æŒçš„ç±»åˆ«ï¼š
- All Beauty
- Video_Games  
- Handmade_product

æ ¹æ®æµ‹è¯•æ•°æ®çš„instructionåˆ—è‡ªåŠ¨åˆ†ç±»å’Œè¯„ä¼°ã€‚

åŠŸèƒ½ç‰¹æ€§ï¼š
1. åˆ†ç±»åˆ«è¯„ä¼°ï¼šæ ¹æ®instructionè‡ªåŠ¨åˆ†ç±»ï¼Œå¯¹æ¯ä¸ªç±»åˆ«åˆ†åˆ«è¯„ä¼°
2. æ¶ˆèå®éªŒï¼šå¯¹æ¯”ä½¿ç”¨å’Œä¸ä½¿ç”¨é¢„è®­ç»ƒCFæ¨¡å‹çš„æ•ˆæœ
3. è¯¦ç»†æŠ¥å‘Šï¼šç”ŸæˆJSONå’Œå¯è¯»æ€§æ–‡æœ¬ä¸¤ç§æ ¼å¼çš„æŠ¥å‘Š

ç”¨æ³•ï¼š
1. æ™®é€šåˆ†ç±»åˆ«è¯„ä¼°ï¼ˆå®Œæ•´æ¨¡å‹ï¼‰ï¼š
   python eval_collm_mf_din.py --checkpoint-path /path/to/checkpoint.pth

2. æ™®é€šåˆ†ç±»åˆ«è¯„ä¼°ï¼ˆç¦ç”¨CFæ¨¡å‹ï¼‰ï¼š
   python eval_collm_mf_din.py --checkpoint-path /path/to/checkpoint.pth --disable-cf

3. æ¶ˆèå®éªŒï¼ˆå¯¹æ¯”ä¸¤ç§æ¨¡å‹ï¼‰ï¼š
   python eval_collm_mf_din.py --checkpoint-path /path/to/checkpoint.pth --ablation

è¾“å‡ºæ–‡ä»¶ï¼š
æ™®é€šè¯„ä¼°æ¨¡å¼ï¼š
- eval_results_by_category_[checkpoint_name]_[timestamp].json: è¯¦ç»†çš„JSONæ ¼å¼ç»“æœ
- eval_summary_[checkpoint_name]_[timestamp].txt: å¯è¯»æ€§å¥½çš„æ±‡æ€»æŠ¥å‘Š

æ¶ˆèå®éªŒæ¨¡å¼ï¼š
- ablation_results_[checkpoint_name]_[timestamp].json: è¯¦ç»†çš„æ¶ˆèå®éªŒç»“æœ
- ablation_summary_[checkpoint_name]_[timestamp].txt: æ¶ˆèå®éªŒå¯¹æ¯”æŠ¥å‘Š

æ¶ˆèå®éªŒä¼šäº§ç”Ÿ6ç§ç»“æœï¼š3ä¸ªç±»åˆ« Ã— 2ç§æ¨¡å‹ç‰ˆæœ¬ï¼ˆå®Œæ•´/æ¶ˆèï¼‰
"""

import argparse
import os
import pickle
import json
import time
from pathlib import Path

import numpy as np
import pandas as pd
import torch
import torch.backends.cudnn as cudnn
from sklearn.metrics import mean_squared_error, mean_absolute_error

# ğŸ”§ å…¼å®¹æ€§è¡¥ä¸ï¼šå¿…é¡»åœ¨å¯¼å…¥ minigpt4 ä¹‹å‰åº”ç”¨
def apply_huggingface_compatibility_patch():
    """åº”ç”¨ HuggingFace å…¼å®¹æ€§è¡¥ä¸"""
    print("ğŸ”§ åº”ç”¨ HuggingFace å…¼å®¹æ€§è¡¥ä¸...")
    
    # æ­¥éª¤1: è®¾ç½®ç¯å¢ƒå˜é‡
    cache_base = os.path.join(os.path.expanduser("~"), ".cache", "huggingface")
    hub_cache = os.path.join(cache_base, "hub")
    
    env_vars = {
        'HF_HOME': cache_base,
        'HUGGINGFACE_HUB_CACHE': hub_cache,
        'TRANSFORMERS_CACHE': os.path.join(cache_base, "transformers"),
        'HF_DATASETS_CACHE': os.path.join(cache_base, "datasets"),
    }
    
    for key, value in env_vars.items():
        if key not in os.environ:
            os.environ[key] = value
    
    # æ­¥éª¤2: ä¿®å¤ huggingface_hub ç‰ˆæœ¬å…¼å®¹æ€§
    try:
        import huggingface_hub
        import sys
        hub_version = huggingface_hub.__version__
        print(f"æ£€æµ‹åˆ° huggingface_hub ç‰ˆæœ¬: {hub_version}")
        
        if hub_version < "0.20.0":
            print("åº”ç”¨åŠ¨æ€å…¼å®¹å±‚...")
            
            # æ·»åŠ ç¼ºå¤±çš„å‡½æ•°
            if not hasattr(huggingface_hub, 'split_torch_state_dict_into_shards'):
                def split_torch_state_dict_into_shards(*args, **kwargs):
                    """å…¼å®¹å±‚ï¼šç®€å•è¿”å›åŸå§‹çŠ¶æ€å­—å…¸"""
                    if args:
                        return {'model.safetensors': args[0]}
                    return {}
                
                huggingface_hub.split_torch_state_dict_into_shards = split_torch_state_dict_into_shards
                print("âœ… æ·»åŠ  split_torch_state_dict_into_shards å…¼å®¹å‡½æ•°")
            
            # æ·»åŠ ç¼ºå¤±çš„ errors æ¨¡å—
            if not hasattr(huggingface_hub, 'errors'):
                # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ errors æ¨¡å—
                class ErrorsModule:
                    class HFValidationError(Exception):
                        pass
                    
                    class LocalEntryNotFoundError(Exception):
                        pass
                    
                    class EntryNotFoundError(Exception):
                        pass
                    
                    class RepositoryNotFoundError(Exception):
                        pass
                    
                    class RevisionNotFoundError(Exception):
                        pass
                
                errors_module = ErrorsModule()
                huggingface_hub.errors = errors_module
                
                # åŒæ—¶åœ¨ sys.modules ä¸­æ³¨å†Œ
                sys.modules['huggingface_hub.errors'] = errors_module
                print("âœ… æ·»åŠ  huggingface_hub.errors å…¼å®¹æ¨¡å—")
            
            # ä¼ªè£…ç‰ˆæœ¬å·
            original_version = huggingface_hub.__version__
            huggingface_hub.__version__ = "0.20.0"
            print(f"ç‰ˆæœ¬ä¼ªè£…: {original_version} -> {huggingface_hub.__version__}")
        
    except Exception as e:
        print(f"å…¼å®¹å±‚åº”ç”¨å¤±è´¥: {e}")
    
    # æ­¥éª¤3: ä¿®å¤ transformers å’Œ PEFT ç‰ˆæœ¬å…¼å®¹æ€§
    try:
        import transformers
        transformers_version = transformers.__version__
        print(f"æ£€æµ‹åˆ° transformers ç‰ˆæœ¬: {transformers_version}")
        
        # æ·»åŠ ç¼ºå¤±çš„ Cache ç›¸å…³ç±»
        if not hasattr(transformers, 'EncoderDecoderCache'):
            # åˆ›å»ºå…¼å®¹çš„ Cache ç±»
            class DummyCache:
                def __init__(self, *args, **kwargs):
                    pass
                
                def update(self, *args, **kwargs):
                    return None
                
                def get_seq_length(self, *args, **kwargs):
                    return 0
            
            # æ·»åŠ ç¼ºå¤±çš„ Cache ç±»
            if not hasattr(transformers, 'Cache'):
                transformers.Cache = DummyCache
                print("âœ… æ·»åŠ  transformers.Cache å…¼å®¹ç±»")
            
            if not hasattr(transformers, 'DynamicCache'):
                transformers.DynamicCache = DummyCache
                print("âœ… æ·»åŠ  transformers.DynamicCache å…¼å®¹ç±»")
                
            if not hasattr(transformers, 'EncoderDecoderCache'):
                transformers.EncoderDecoderCache = DummyCache
                print("âœ… æ·»åŠ  transformers.EncoderDecoderCache å…¼å®¹ç±»")
                
            if not hasattr(transformers, 'HybridCache'):
                transformers.HybridCache = DummyCache
                print("âœ… æ·»åŠ  transformers.HybridCache å…¼å®¹ç±»")
        
    except Exception as e:
        print(f"transformers å…¼å®¹å±‚å¤±è´¥: {e}")
    
    # æ­¥éª¤4: ä¿®å¤ PEFT utils.config æ¨¡å—ç¼ºå¤±
    try:
        import peft
        import peft.utils
        
        # æ£€æŸ¥ peft.utils.config æ˜¯å¦å­˜åœ¨
        if not hasattr(peft.utils, 'config'):
            # åˆ›å»ºå…¼å®¹çš„ config æ¨¡å—
            class ConfigModule:
                class PeftConfigMixin:
                    def __init__(self, *args, **kwargs):
                        pass
                    
                    def to_dict(self):
                        return {}
                    
                    @classmethod
                    def from_dict(cls, config_dict):
                        return cls()
            
            config_module = ConfigModule()
            peft.utils.config = config_module
            
            # åŒæ—¶åœ¨ sys.modules ä¸­æ³¨å†Œ
            sys.modules['peft.utils.config'] = config_module
            print("âœ… æ·»åŠ  peft.utils.config å…¼å®¹æ¨¡å—")
        
    except Exception as e:
        print(f"PEFT utils.config å…¼å®¹å±‚å¤±è´¥: {e}")
    
    # æ­¥éª¤5: å…¼å®¹ torchvision/PIL æ’å€¼å¸¸é‡ (NEAREST_EXACT)
    try:
        from PIL import Image as _PILImage
        # ç¡®ä¿å­˜åœ¨ Resamplingï¼Œå¹¶æä¾› NEAREST_EXACT å…¼å®¹åˆ«å
        if not hasattr(_PILImage, 'Resampling'):
            class _Resampling:
                NEAREST = _PILImage.NEAREST
                BILINEAR = _PILImage.BILINEAR
                BICUBIC = _PILImage.BICUBIC
                LANCZOS = getattr(_PILImage, 'LANCZOS', _PILImage.BICUBIC)
                BOX = getattr(_PILImage, 'BOX', _PILImage.NEAREST)
                HAMMING = getattr(_PILImage, 'HAMMING', _PILImage.NEAREST)
                NEAREST_EXACT = _PILImage.NEAREST
            _PILImage.Resampling = _Resampling
            print("âœ… æ·»åŠ  PIL.Image.Resampling å…¼å®¹ç±»")
        else:
            if not hasattr(_PILImage.Resampling, 'NEAREST_EXACT'):
                _PILImage.Resampling.NEAREST_EXACT = _PILImage.Resampling.NEAREST
                print("âœ… ä¸º PIL.Image.Resampling æ·»åŠ  NEAREST_EXACT å…¼å®¹åˆ«å")
    except Exception as e:
        print(f"PILå…¼å®¹å±‚å¤±è´¥: {e}")

    try:
        from torchvision.transforms import InterpolationMode as _IM
        if not hasattr(_IM, 'NEAREST_EXACT'):
            # å®šä¹‰åˆ«åï¼Œé¿å… transformers ä¾èµ–å¤±è´¥
            _IM.NEAREST_EXACT = _IM.NEAREST
            print("âœ… æ·»åŠ  torchvision.InterpolationMode.NEAREST_EXACT å…¼å®¹åˆ«å")
    except Exception as e:
        print(f"torchvisionå…¼å®¹å±‚å¤±è´¥: {e}")

    print("âœ… å…¼å®¹æ€§è¡¥ä¸åº”ç”¨å®Œæˆ")
    return True

# ç«‹å³åº”ç”¨è¡¥ä¸
apply_huggingface_compatibility_patch()

import minigpt4.tasks as tasks
from minigpt4.common.config import Config
from minigpt4.common.dist_utils import get_rank, init_distributed_mode
from minigpt4.common.logger import setup_logger
from minigpt4.common.registry import registry
from minigpt4.common.utils import now

# imports modules for registration
from minigpt4.datasets.builders import *
from minigpt4.models import *
from minigpt4.processors import *
from minigpt4.runners import *
from minigpt4.tasks import *


def calculate_rmse_mae(user, predict, rating):
    """è®¡ç®—ç”¨æˆ·çº§RMSEå’ŒMAEæŒ‡æ ‡"""
    if not isinstance(predict, np.ndarray):
        predict = np.array(predict)
    if not isinstance(rating, np.ndarray):
        rating = np.array(rating)
    if not isinstance(user, np.ndarray):
        user = np.array(user)
        
    predict = predict.squeeze()
    rating = rating.squeeze()
    user = user.squeeze()

    start_time = time.time()
    u, inverse, counts = np.unique(user, return_inverse=True, return_counts=True)
    index = np.argsort(inverse)
    candidates_dict = {}
    k = 0
    total_num = 0
    only_one_interaction = 0
    computed_u = []
    
    for u_i in u:
        start_id, end_id = total_num, total_num + counts[k]
        u_i_counts = counts[k]
        index_ui = index[start_id:end_id]
        if u_i_counts == 1:
            only_one_interaction += 1
            total_num += counts[k]
            k += 1
            continue
        candidates_dict[u_i] = [predict[index_ui], rating[index_ui]]
        total_num += counts[k]
        k += 1
    
    print(f"åªæœ‰ä¸€ä¸ªäº¤äº’çš„ç”¨æˆ·æ•°: {only_one_interaction}")
    user_rmse = []
    user_mae = []
    
    for ui, pre_and_true in candidates_dict.items():
        pre_i, rating_i = pre_and_true
        ui_rmse = np.sqrt(mean_squared_error(rating_i, pre_i))
        ui_mae = mean_absolute_error(rating_i, pre_i)
        user_rmse.append(ui_rmse)
        user_mae.append(ui_mae)
        computed_u.append(ui)
    
    user_rmse = np.array(user_rmse)
    user_mae = np.array(user_mae)
    print(f"è®¡ç®—çš„ç”¨æˆ·æ•°: {user_rmse.shape[0]}")
    avg_rmse = user_rmse.mean()
    avg_mae = user_mae.mean()
    print(f"ç”¨æˆ·çº§ RMSE: {avg_rmse:.4f}, ç”¨æˆ·çº§ MAE: {avg_mae:.4f}, è€—æ—¶: {time.time() - start_time:.2f}s")
    return avg_rmse, avg_mae, computed_u, user_rmse, user_mae


def categorize_data_by_instruction(test_data):
    """æ ¹æ®instructionå°†æ•°æ®åˆ†ä¸ºä¸‰ä¸ªç±»åˆ«"""
    print("\nğŸ“Š æ ¹æ®instructionåˆ†ç±»æ•°æ®...")
    
    # å®šä¹‰ç±»åˆ«å…³é”®è¯æ˜ å°„
    category_mapping = {
        'All Beauty': 'All Beauty',
        'Video_Games': 'Video_Games', 
        'Handmade_product': 'Handmade_product'
    }
    
    # åˆ†ç±»æ•°æ®
    categorized_data = {}
    
    for category_key, category_name in category_mapping.items():
        # ç­›é€‰åŒ…å«å¯¹åº”ç±»åˆ«å…³é”®è¯çš„æ•°æ®
        mask = test_data['instruction'].str.contains(category_key, case=False, na=False)
        category_data = test_data[mask].copy()
        
        if len(category_data) > 0:
            categorized_data[category_name] = category_data
            print(f"âœ… {category_name}: {len(category_data)} æ¡æ•°æ®")
            print(f"   ç”¨æˆ·æ•°: {category_data['user_id'].nunique()}")
            print(f"   ç‰©å“æ•°: {category_data['asin'].nunique()}")
            print(f"   è¯„åˆ†èŒƒå›´: {category_data['rating'].min():.1f} - {category_data['rating'].max():.1f}")
        else:
            print(f"âš ï¸ {category_name}: æœªæ‰¾åˆ°æ•°æ®")
    
    # éªŒè¯åˆ†ç±»ç»“æœ
    total_categorized = sum(len(data) for data in categorized_data.values())
    print(f"\nğŸ“ˆ åˆ†ç±»ç»Ÿè®¡:")
    print(f"åŸå§‹æ•°æ®æ€»æ•°: {len(test_data)}")
    print(f"åˆ†ç±»åæ€»æ•°: {total_categorized}")
    print(f"åˆ†ç±»è¦†ç›–ç‡: {total_categorized/len(test_data)*100:.2f}%")
    
    return categorized_data


def load_test_data(test_data_path):
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®: {test_data_path}")
    
    if not os.path.exists(test_data_path):
        raise FileNotFoundError(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {test_data_path}")
    
    with open(test_data_path, 'rb') as f:
        test_data = pickle.load(f)
    
    if isinstance(test_data, pd.DataFrame):
        print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {test_data.shape}")
        print(f"æ•°æ®åˆ—: {list(test_data.columns)}")
        
        # æ£€æŸ¥å¹¶æ˜ å°„å­—æ®µå
        field_mapping = {
            'uid': 'user_id',
            'iid': 'asin'
        }
        
        # åº”ç”¨å­—æ®µæ˜ å°„
        for old_name, new_name in field_mapping.items():
            if old_name in test_data.columns and new_name not in test_data.columns:
                test_data = test_data.rename(columns={old_name: new_name})
                print(f"ğŸ”„ å­—æ®µæ˜ å°„: {old_name} -> {new_name}")
        
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        required_fields = ['user_id', 'asin', 'rating', 'instruction']
        missing_fields = [field for field in required_fields if field not in test_data.columns]
        if missing_fields:
            print(f"âš ï¸ ç¼ºå°‘å­—æ®µ: {missing_fields}")
            # å¦‚æœä»ç„¶ç¼ºå°‘å­—æ®µï¼Œå°è¯•æ˜¾ç¤ºå¯ç”¨å­—æ®µ
            print(f"å¯ç”¨å­—æ®µ: {list(test_data.columns)}")
        
        print(f"ç”¨æˆ·æ•°: {test_data['user_id'].nunique()}")
        print(f"ç‰©å“æ•°: {test_data['asin'].nunique()}")
        print(f"è¯„åˆ†èŒƒå›´: {test_data['rating'].min():.1f} - {test_data['rating'].max():.1f}")
        
        # æ˜¾ç¤ºinstructionçš„åˆ†å¸ƒæƒ…å†µ
        print(f"\nğŸ“‹ Instructionåˆ†å¸ƒ:")
        instruction_counts = test_data['instruction'].value_counts()
        for instruction, count in instruction_counts.items():
            print(f"  '{instruction}': {count:,}æ¬¡")
        
        return test_data
    else:
        raise ValueError(f"æœŸæœ›DataFrameæ ¼å¼ï¼Œä½†å¾—åˆ°: {type(test_data)}")


def load_checkpoint_and_model(checkpoint_path, cfg, disable_cf_model=False):
    """åŠ è½½checkpointå’Œæ¨¡å‹"""
    cf_status = "ä¸ä½¿ç”¨CFæ¨¡å‹" if disable_cf_model else "ä½¿ç”¨CFæ¨¡å‹"
    print(f"ğŸ“‚ åŠ è½½checkpoint: {checkpoint_path} ({cf_status})")
    
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location="cpu")
    print(f"âœ… CheckpointåŠ è½½æˆåŠŸï¼ŒåŒ…å«é”®: {list(checkpoint.keys())}")
    
    if 'epoch' in checkpoint:
        print(f"è®­ç»ƒepoch: {checkpoint['epoch']}")
    
    # åˆ›å»ºä»»åŠ¡å’Œæ¨¡å‹
    print("ğŸ”§ åˆå§‹åŒ–ä»»åŠ¡å’Œæ¨¡å‹...")
    task = tasks.setup_task(cfg)
    
    # å¦‚æœéœ€è¦æ•°æ®é›†æ¥è·å–ç”¨æˆ·/ç‰©å“æ•°é‡ï¼Œå…ˆæ„å»ºæ•°æ®é›†
    datasets = task.build_datasets(cfg)
    
    # ä»æ•°æ®é›†è·å–ç”¨æˆ·å’Œç‰©å“æ•°é‡
    try:
        data_name = list(datasets.keys())[0]
        split_dict = datasets[data_name]
        user_num = -1
        item_num = -1
        for split_key, ds in split_dict.items():
            try:
                user_num = max(user_num, int(getattr(ds, 'user_num', -1)))
                item_num = max(item_num, int(getattr(ds, 'item_num', -1)))
            except Exception:
                continue
        if user_num <= 0 or item_num <= 0:
            raise RuntimeError("invalid user/item num from datasets")
    except Exception:
        # å›é€€ï¼šä½¿ç”¨é…ç½®ä¸­çš„å€¼
        user_num = cfg.model_cfg.rec_config.get('user_num', -100)
        item_num = cfg.model_cfg.rec_config.get('item_num', -100)

    cfg.model_cfg.rec_config.user_num = int(user_num)
    cfg.model_cfg.rec_config.item_num = int(item_num)
    
    print(f"ç”¨æˆ·æ•°: {user_num}, ç‰©å“æ•°: {item_num}")
    
    # æ¶ˆèå®éªŒï¼šå¦‚æœç¦ç”¨CFæ¨¡å‹ï¼Œä¿®æ”¹é…ç½®
    if disable_cf_model:
        print("ğŸš« æ¶ˆèå®éªŒï¼šç¦ç”¨CFæ¨¡å‹ç»„ä»¶")
        # å¤‡ä»½åŸå§‹é…ç½®
        original_cf_config = getattr(cfg.model_cfg, 'use_cf_model', True)
        # è®¾ç½®ä¸ä½¿ç”¨CFæ¨¡å‹
        cfg.model_cfg.use_cf_model = False
        if hasattr(cfg.model_cfg.rec_config, 'use_pretrained_cf'):
            cfg.model_cfg.rec_config.use_pretrained_cf = False
        if hasattr(cfg.model_cfg.rec_config, 'enable_cf_component'):
            cfg.model_cfg.rec_config.enable_cf_component = False
        print("âœ… CFæ¨¡å‹ç»„ä»¶å·²ç¦ç”¨")
    
    # æ„å»ºæ¨¡å‹
    model = task.build_model(cfg)
    
    # åŠ è½½æ¨¡å‹æƒé‡
    try:
        # å¦‚æœç¦ç”¨CFæ¨¡å‹ï¼Œè¿‡æ»¤æ‰CFç›¸å…³çš„æƒé‡
        if disable_cf_model:
            print("ğŸ”§ è¿‡æ»¤CFæ¨¡å‹ç›¸å…³æƒé‡...")
            state_dict = checkpoint["model"]
            filtered_state_dict = {}
            cf_related_keys = []
            
            for key, value in state_dict.items():
                # è·³è¿‡CFæ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ˆæ ¹æ®å®é™…æ¨¡å‹ç»“æ„è°ƒæ•´ï¼‰
                if any(cf_keyword in key.lower() for cf_keyword in ['cf_model', 'collaborative', 'mf_', 'matrix_fact']):
                    cf_related_keys.append(key)
                    continue
                filtered_state_dict[key] = value
            
            print(f"   è¿‡æ»¤æ‰ {len(cf_related_keys)} ä¸ªCFæ¨¡å‹ç›¸å…³æƒé‡")
            if cf_related_keys:
                print(f"   è¿‡æ»¤çš„æƒé‡é”®: {cf_related_keys[:5]}{'...' if len(cf_related_keys) > 5 else ''}")
            
            missing_keys, unexpected_keys = model.load_state_dict(filtered_state_dict, strict=False)
            print(f"âœ… æ¶ˆèæ¨¡å‹æƒé‡åŠ è½½å®Œæˆ (ç¼ºå°‘: {len(missing_keys)}, æ„å¤–: {len(unexpected_keys)})")
        else:
            model.load_state_dict(checkpoint["model"], strict=False)
            print("âœ… å®Œæ•´æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥ï¼Œå°è¯•å®½æ¾åŠ è½½: {e}")
        missing_keys, unexpected_keys = model.load_state_dict(checkpoint["model"], strict=False)
        print(f"ç¼ºå°‘çš„é”®: {len(missing_keys)}, æ„å¤–çš„é”®: {len(unexpected_keys)}")
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # è®¾ç½®æ¨¡å‹è¿è¡Œæ¨¡å¼ï¼ˆå¿…é¡»è®¾ç½®ï¼Œå¦åˆ™forwardä¼šæŠ¥é”™ï¼‰
    mode = cfg.run_cfg.get('mode', 'v2')
    model.set_mode(mode)
    print(f"âœ… æ¨¡å‹è¿è¡Œæ¨¡å¼è®¾ç½®ä¸º: {mode}")
    
    return model, task, datasets


def create_dataloader_for_category(category_data, test_dataset, cfg):
    """ä¸ºç‰¹å®šç±»åˆ«çš„æ•°æ®åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    from torch.utils.data import DataLoader, Subset
    import numpy as np
    
    # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„æ•°æ®é›†å®ç°æ¥åˆ›å»ºå­é›†
    # ç”±äºæˆ‘ä»¬æ— æ³•ç›´æ¥ä»å¤–éƒ¨æ•°æ®åˆ›å»ºDataLoaderï¼Œ
    # æˆ‘ä»¬éœ€è¦ä½¿ç”¨ç°æœ‰çš„test_datasetçš„ç»“æ„
    
    batch_size = cfg.run_cfg.get('batch_size_eval', 4)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„åŒ…è£…å™¨
    class CategoryDataLoader:
        def __init__(self, category_data, original_dataset):
            self.category_data = category_data
            self.original_dataset = original_dataset
            self.batch_size = batch_size
            
        def __iter__(self):
            # è¿™é‡Œæˆ‘ä»¬éœ€è¦å®ç°æŒ‰ç±»åˆ«ç­›é€‰çš„è¿­ä»£é€»è¾‘
            # æš‚æ—¶è¿”å›åŸå§‹æ•°æ®é›†çš„è¿­ä»£å™¨
            # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„æ•°æ®é›†ç»“æ„æ¥å®ç°
            return iter(DataLoader(
                self.original_dataset,
                batch_size=self.batch_size,
                shuffle=False,
                num_workers=cfg.run_cfg.get('num_workers', 2),
                collate_fn=getattr(self.original_dataset, 'collater', None)
            ))
        
        def __len__(self):
            return len(self.category_data)
    
    return CategoryDataLoader(category_data, test_dataset)


def evaluate_single_category(model, task, category_name, category_dataloader, device):
    """è¯„ä¼°å•ä¸ªç±»åˆ«"""
    print(f"\nğŸ”¬ è¯„ä¼°ç±»åˆ«: {category_name}")
    
    # åˆ›å»ºdata_loaderså¯¹è±¡ï¼ˆåŒ…è£…DataLoaderï¼‰
    class DataLoaders:
        def __init__(self, loader):
            self.loaders = [loader]
    
    data_loaders = DataLoaders(category_dataloader)
    
    # ä½¿ç”¨ä»»åŠ¡çš„evaluationæ–¹æ³•
    with torch.no_grad():
        eval_results = task.evaluation(
            model=model, 
            data_loaders=data_loaders, 
            cuda_enabled=torch.cuda.is_available(),
            split_name=f"test_{category_name}"
        )
    
    print(f"âœ… {category_name} è¯„ä¼°å®Œæˆï¼Œç»“æœ: {eval_results}")
    
    # å¤„ç†è¯„ä¼°ç»“æœ
    if eval_results is not None:
        final_results = task.after_evaluation(
            val_result=eval_results,
            split_name=f"test_{category_name}",
            epoch="final"
        )
    else:
        final_results = {"error": f"{category_name} è¯„ä¼°å¤±è´¥ï¼Œç»“æœä¸ºNone"}
    
    return final_results


def run_single_model_evaluation(model, task, datasets, cfg, test_data_path, model_type="full"):
    """è¿è¡Œå•ä¸ªæ¨¡å‹çš„åˆ†ç±»åˆ«è¯„ä¼°"""
    print(f"\nğŸ”¬ å¼€å§‹åˆ†ç±»åˆ«è¯„ä¼° ({model_type}æ¨¡å‹)...")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    
    print(f"è®¾å¤‡: {device}")
    
    # 1. åŠ è½½å¤–éƒ¨æµ‹è¯•æ•°æ®
    test_data = load_test_data(test_data_path)
    
    # 2. æŒ‰instructionåˆ†ç±»æ•°æ®
    categorized_data = categorize_data_by_instruction(test_data)
    
    if not categorized_data:
        raise RuntimeError("æœªèƒ½æˆåŠŸåˆ†ç±»æµ‹è¯•æ•°æ®")
    
    # 3. è·å–åŸå§‹æµ‹è¯•æ•°æ®é›†ï¼ˆç”¨äºåˆ›å»ºæ•°æ®åŠ è½½å™¨çš„ç»“æ„ï¼‰
    test_dataset = None
    for data_name, split_dict in datasets.items():
        if 'test' in split_dict:
            test_dataset = split_dict['test']
            print(f"âœ… ä½¿ç”¨å†…ç½®æµ‹è¯•é›†ç»“æ„: {data_name}/test")
            break
    
    if test_dataset is None:
        print("âš ï¸ æœªæ‰¾åˆ°å†…ç½®æµ‹è¯•é›†ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ•°æ®é›†")
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ•°æ®é›†
        for data_name, split_dict in datasets.items():
            for split_name, dataset in split_dict.items():
                test_dataset = dataset
                print(f"âœ… ä½¿ç”¨æ•°æ®é›†ç»“æ„: {data_name}/{split_name}")
                break
            if test_dataset is not None:
                break
    
    if test_dataset is None:
        raise RuntimeError("æœªæ‰¾åˆ°å¯ç”¨çš„æ•°æ®é›†ç»“æ„")
    
    # 4. åˆ†ç±»åˆ«è¿›è¡Œè¯„ä¼°
    category_results = {}
    overall_stats = {
        'total_samples': 0,
        'total_categories': len(categorized_data),
        'model_type': model_type
    }
    
    print(f"\nğŸš€ å¼€å§‹è¯„ä¼° {len(categorized_data)} ä¸ªç±»åˆ«...")
    
    # å…ˆè¿›è¡Œæ•´ä½“è¯„ä¼°ï¼ˆå¦‚æœéœ€è¦ï¼‰
    print(f"\nğŸ“Š æ•´ä½“è¯„ä¼° ({model_type}æ¨¡å‹)...")
    
    # åˆ›å»ºå®Œæ•´æ•°æ®çš„æ•°æ®åŠ è½½å™¨
    from torch.utils.data import DataLoader
    batch_size = cfg.run_cfg.get('batch_size_eval', 4)
    full_dataloader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=cfg.run_cfg.get('num_workers', 2),
        collate_fn=getattr(test_dataset, 'collater', None)
    )
    
    # æ•´ä½“è¯„ä¼°
    class DataLoaders:
        def __init__(self, loader):
            self.loaders = [loader]
    
    data_loaders = DataLoaders(full_dataloader)
    
    with torch.no_grad():
        overall_eval_results = task.evaluation(
            model=model, 
            data_loaders=data_loaders, 
            cuda_enabled=torch.cuda.is_available(),
            split_name=f"test_overall_{model_type}"
        )
    
    if overall_eval_results is not None:
        overall_results = task.after_evaluation(
            val_result=overall_eval_results,
            split_name=f"test_overall_{model_type}",
            epoch="final"
        )
    else:
        overall_results = {"error": f"æ•´ä½“è¯„ä¼°å¤±è´¥ï¼Œç»“æœä¸ºNone ({model_type}æ¨¡å‹)"}
    
    category_results['Overall'] = overall_results
    overall_stats['total_samples'] = len(test_data)
    
    # åˆ†ç±»åˆ«è¯„ä¼°
    for category_name, category_data in categorized_data.items():
        print(f"\n{'='*50}")
        print(f"ğŸ¯ è¯„ä¼°ç±»åˆ«: {category_name} ({model_type}æ¨¡å‹)")
        print(f"æ•°æ®é‡: {len(category_data)}")
        print(f"{'='*50}")
        
        # åˆ›å»ºç±»åˆ«æ•°æ®åŠ è½½å™¨
        category_dataloader = create_dataloader_for_category(category_data, test_dataset, cfg)
        
        # è¯„ä¼°è¯¥ç±»åˆ«
        try:
            category_result = evaluate_single_category(
                model, task, f"{category_name}_{model_type}", category_dataloader, device
            )
            category_results[category_name] = category_result
            
            # è®¡ç®—è¯¥ç±»åˆ«çš„ç»Ÿè®¡ä¿¡æ¯
            print(f"âœ… {category_name} ({model_type}æ¨¡å‹) è¯„ä¼°å®Œæˆ")
            if isinstance(category_result, dict):
                for key, value in category_result.items():
                    if isinstance(value, (int, float)):
                        print(f"   {key}: {value:.6f}")
                    else:
                        print(f"   {key}: {value}")
            
        except Exception as e:
            print(f"âŒ {category_name} ({model_type}æ¨¡å‹) è¯„ä¼°å¤±è´¥: {e}")
            category_results[category_name] = {"error": str(e)}
        
        print(f"{'='*50}")
    
    # 5. æ±‡æ€»ç»“æœ
    final_results = {
        'overall_stats': overall_stats,
        'category_results': category_results,
        'evaluation_summary': {
            'total_categories_evaluated': len(category_results) - 1,  # é™¤å»Overall
            'successful_evaluations': sum(1 for k, v in category_results.items() 
                                        if k != 'Overall' and 'error' not in v),
            'failed_evaluations': sum(1 for k, v in category_results.items() 
                                    if k != 'Overall' and 'error' in v),
            'model_type': model_type
        }
    }
    
    print(f"\nğŸ“Š {model_type}æ¨¡å‹è¯„ä¼°æ±‡æ€»:")
    print(f"æ€»ç±»åˆ«æ•°: {final_results['evaluation_summary']['total_categories_evaluated']}")
    print(f"æˆåŠŸè¯„ä¼°: {final_results['evaluation_summary']['successful_evaluations']}")
    print(f"å¤±è´¥è¯„ä¼°: {final_results['evaluation_summary']['failed_evaluations']}")
    
    return final_results


def run_ablation_evaluation(checkpoint_path, cfg, test_data_path, output_dir):
    """è¿è¡Œæ¶ˆèå®éªŒè¯„ä¼°"""
    print("\nğŸ§ª å¼€å§‹æ¶ˆèå®éªŒè¯„ä¼°...")
    print("å°†å¯¹æ¯”ä»¥ä¸‹ä¸¤ç§æ¨¡å‹é…ç½®ï¼š")
    print("1. å®Œæ•´æ¨¡å‹ (åŒ…å«é¢„è®­ç»ƒCFæ¨¡å‹)")
    print("2. æ¶ˆèæ¨¡å‹ (ä¸ä½¿ç”¨é¢„è®­ç»ƒCFæ¨¡å‹)")
    
    all_results = {}
    
    # 1. è¯„ä¼°å®Œæ•´æ¨¡å‹
    print("\n" + "="*80)
    print("ğŸ”µ ç¬¬ä¸€é˜¶æ®µï¼šè¯„ä¼°å®Œæ•´æ¨¡å‹ (åŒ…å«é¢„è®­ç»ƒCFæ¨¡å‹)")
    print("="*80)
    
    try:
        # åŠ è½½å®Œæ•´æ¨¡å‹
        full_model, task, datasets = load_checkpoint_and_model(checkpoint_path, cfg, disable_cf_model=False)
        
        # è¿è¡Œå®Œæ•´æ¨¡å‹è¯„ä¼°
        full_results = run_single_model_evaluation(
            full_model, task, datasets, cfg, test_data_path, model_type="full"
        )
        all_results['full_model'] = full_results
        print("âœ… å®Œæ•´æ¨¡å‹è¯„ä¼°å®Œæˆ")
        
        # é‡Šæ”¾GPUå†…å­˜
        del full_model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
    except Exception as e:
        print(f"âŒ å®Œæ•´æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
        all_results['full_model'] = {"error": str(e)}
    
    # 2. è¯„ä¼°æ¶ˆèæ¨¡å‹
    print("\n" + "="*80)
    print("ğŸ”´ ç¬¬äºŒé˜¶æ®µï¼šè¯„ä¼°æ¶ˆèæ¨¡å‹ (ä¸ä½¿ç”¨é¢„è®­ç»ƒCFæ¨¡å‹)")
    print("="*80)
    
    try:
        # åŠ è½½æ¶ˆèæ¨¡å‹
        ablation_model, task, datasets = load_checkpoint_and_model(checkpoint_path, cfg, disable_cf_model=True)
        
        # è¿è¡Œæ¶ˆèæ¨¡å‹è¯„ä¼°
        ablation_results = run_single_model_evaluation(
            ablation_model, task, datasets, cfg, test_data_path, model_type="ablation"
        )
        all_results['ablation_model'] = ablation_results
        print("âœ… æ¶ˆèæ¨¡å‹è¯„ä¼°å®Œæˆ")
        
        # é‡Šæ”¾GPUå†…å­˜
        del ablation_model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
    except Exception as e:
        print(f"âŒ æ¶ˆèæ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
        all_results['ablation_model'] = {"error": str(e)}
    
    # 3. è®¡ç®—æ¯”è¾ƒç»“æœ
    print("\n" + "="*80)
    print("ğŸ“Š ç¬¬ä¸‰é˜¶æ®µï¼šè®¡ç®—å¯¹æ¯”ç»“æœ")
    print("="*80)
    
    comparison_results = generate_comparison_results(all_results)
    all_results['comparison'] = comparison_results
    
    return all_results


def generate_comparison_results(all_results):
    """ç”Ÿæˆä¸¤ç§æ¨¡å‹çš„å¯¹æ¯”ç»“æœ"""
    print("ğŸ” ç”Ÿæˆæ¨¡å‹å¯¹æ¯”åˆ†æ...")
    
    comparison = {
        'summary': {},
        'category_comparison': {},
        'improvement_analysis': {}
    }
    
    # æ£€æŸ¥æ˜¯å¦ä¸¤ç§æ¨¡å‹éƒ½è¯„ä¼°æˆåŠŸ
    full_results = all_results.get('full_model')
    ablation_results = all_results.get('ablation_model')
    
    if not full_results or 'error' in full_results:
        comparison['summary']['full_model_error'] = full_results.get('error', 'å®Œæ•´æ¨¡å‹è¯„ä¼°å¤±è´¥')
        return comparison
    
    if not ablation_results or 'error' in ablation_results:
        comparison['summary']['ablation_model_error'] = ablation_results.get('error', 'æ¶ˆèæ¨¡å‹è¯„ä¼°å¤±è´¥')
        return comparison
    
    # è·å–ç±»åˆ«ç»“æœ
    full_categories = full_results.get('category_results', {})
    ablation_categories = ablation_results.get('category_results', {})
    
    # å¯¹æ¯”å„ç±»åˆ«ç»“æœ
    for category in full_categories.keys():
        if category in ablation_categories:
            full_result = full_categories[category]
            ablation_result = ablation_categories[category]
            
            if 'error' not in full_result and 'error' not in ablation_result:
                category_comparison = {}
                
                # æ¯”è¾ƒæ•°å€¼æŒ‡æ ‡
                for metric in full_result.keys():
                    if isinstance(full_result.get(metric), (int, float)) and isinstance(ablation_result.get(metric), (int, float)):
                        full_value = full_result[metric]
                        ablation_value = ablation_result[metric]
                        
                        # è®¡ç®—æ”¹è¿›å¹…åº¦
                        if ablation_value != 0:
                            improvement = ((full_value - ablation_value) / abs(ablation_value)) * 100
                        else:
                            improvement = float('inf') if full_value > 0 else float('-inf')
                        
                        category_comparison[metric] = {
                            'full_model': full_value,
                            'ablation_model': ablation_value,
                            'improvement_percent': improvement,
                            'better_model': 'full' if full_value > ablation_value else 'ablation' if full_value < ablation_value else 'equal'
                        }
                
                comparison['category_comparison'][category] = category_comparison
            else:
                comparison['category_comparison'][category] = {
                    'error': f"full_error: {full_result.get('error', 'None')}, ablation_error: {ablation_result.get('error', 'None')}"
                }
    
    # ç”Ÿæˆæ”¹è¿›åˆ†ææ‘˜è¦
    improvement_summary = {}
    for category, metrics in comparison['category_comparison'].items():
        if 'error' not in metrics:
            for metric, data in metrics.items():
                if metric not in improvement_summary:
                    improvement_summary[metric] = {
                        'categories_compared': 0,
                        'full_better_count': 0,
                        'ablation_better_count': 0,
                        'equal_count': 0,
                        'average_improvement': 0
                    }
                
                improvement_summary[metric]['categories_compared'] += 1
                
                if data['better_model'] == 'full':
                    improvement_summary[metric]['full_better_count'] += 1
                elif data['better_model'] == 'ablation':
                    improvement_summary[metric]['ablation_better_count'] += 1
                else:
                    improvement_summary[metric]['equal_count'] += 1
                
                if isinstance(data['improvement_percent'], (int, float)) and not np.isinf(data['improvement_percent']):
                    improvement_summary[metric]['average_improvement'] += data['improvement_percent']
    
    # è®¡ç®—å¹³å‡æ”¹è¿›
    for metric in improvement_summary:
        if improvement_summary[metric]['categories_compared'] > 0:
            improvement_summary[metric]['average_improvement'] /= improvement_summary[metric]['categories_compared']
    
    comparison['improvement_analysis'] = improvement_summary
    
    # ç”Ÿæˆæ€»ç»“
    total_comparisons = sum(data['categories_compared'] for data in improvement_summary.values())
    comparison['summary'] = {
        'total_metrics_compared': len(improvement_summary),
        'total_comparisons': total_comparisons,
        'categories_evaluated': len(comparison['category_comparison'])
    }
    
    print(f"âœ… å¯¹æ¯”åˆ†æå®Œæˆï¼š{comparison['summary']['categories_evaluated']} ä¸ªç±»åˆ«ï¼Œ{comparison['summary']['total_metrics_compared']} ä¸ªæŒ‡æ ‡")
    
    return comparison


def save_results(results, output_dir, checkpoint_path, is_ablation=False):
    """ä¿å­˜åˆ†ç±»åˆ«è¯„ä¼°ç»“æœ"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆç»“æœæ–‡ä»¶å
    checkpoint_name = Path(checkpoint_path).stem
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    
    if is_ablation:
        results_file = output_dir / f"ablation_results_{checkpoint_name}_{timestamp}.json"
        summary_file = output_dir / f"ablation_summary_{checkpoint_name}_{timestamp}.txt"
        eval_type = "ablation_study"
        title = "CoRAæ¨¡å‹æ¶ˆèå®éªŒè¯„ä¼°æŠ¥å‘Š"
    else:
        results_file = output_dir / f"eval_results_by_category_{checkpoint_name}_{timestamp}.json"
        summary_file = output_dir / f"eval_summary_{checkpoint_name}_{timestamp}.txt"
        eval_type = "category_wise"
        title = "CoRAæ¨¡å‹åˆ†ç±»åˆ«è¯„ä¼°æŠ¥å‘Š"
    
    # æ·»åŠ å…ƒä¿¡æ¯
    results_with_meta = {
        "evaluation_time": timestamp,
        "checkpoint_path": str(checkpoint_path),
        "evaluation_type": eval_type,
        "results": results
    }
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSON
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results_with_meta, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"ğŸ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # åˆ›å»ºå¯è¯»æ€§æ›´å¥½çš„æ±‡æ€»æŠ¥å‘Š
    with open(summary_file, 'w', encoding='utf-8') as f:
        # å†™å…¥æ±‡æ€»æŠ¥å‘Š
        f.write("=" * 80 + "\n")
        f.write(f"ğŸ“Š {title}\n")
        f.write("=" * 80 + "\n")
        f.write(f"è¯„ä¼°æ—¶é—´: {timestamp}\n")
        f.write(f"Checkpoint: {checkpoint_path}\n")
        f.write(f"è¯„ä¼°ç±»å‹: {eval_type}\n")
        f.write("\n")
        
        if is_ablation:
            # æ¶ˆèå®éªŒæŠ¥å‘Š
            write_ablation_summary(f, results)
        else:
            # æ™®é€šåˆ†ç±»åˆ«æŠ¥å‘Š
            write_category_summary(f, results)
        
        f.write("=" * 80 + "\n")
    
    print(f"ğŸ“„ è¯„ä¼°æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_file}")
    
    # åœ¨æ§åˆ¶å°æ˜¾ç¤ºæ±‡æ€»ä¿¡æ¯
    print_console_summary(results, is_ablation)
    
    return results_file, summary_file


def write_ablation_summary(f, results):
    """å†™å…¥æ¶ˆèå®éªŒæ±‡æ€»æŠ¥å‘Š"""
    # å®éªŒæ¦‚è¿°
    f.write("ğŸ§ª æ¶ˆèå®éªŒæ¦‚è¿°:\n")
    f.write("-" * 40 + "\n")
    f.write("å¯¹æ¯”é…ç½®:\n")
    f.write("  1. å®Œæ•´æ¨¡å‹ (åŒ…å«é¢„è®­ç»ƒCFæ¨¡å‹)\n")
    f.write("  2. æ¶ˆèæ¨¡å‹ (ä¸ä½¿ç”¨é¢„è®­ç»ƒCFæ¨¡å‹)\n")
    f.write("\n")
    
    # æ¨¡å‹è¯„ä¼°ç»“æœ
    if 'full_model' in results:
        f.write("ğŸ”µ å®Œæ•´æ¨¡å‹ç»“æœ:\n")
        f.write("-" * 40 + "\n")
        full_results = results['full_model']
        if 'error' in full_results:
            f.write(f"   âŒ é”™è¯¯: {full_results['error']}\n")
        else:
            write_model_results(f, full_results, "   ")
        f.write("\n")
    
    if 'ablation_model' in results:
        f.write("ğŸ”´ æ¶ˆèæ¨¡å‹ç»“æœ:\n")
        f.write("-" * 40 + "\n")
        ablation_results = results['ablation_model']
        if 'error' in ablation_results:
            f.write(f"   âŒ é”™è¯¯: {ablation_results['error']}\n")
        else:
            write_model_results(f, ablation_results, "   ")
        f.write("\n")
    
    # å¯¹æ¯”åˆ†æ
    if 'comparison' in results:
        f.write("ğŸ“Š å¯¹æ¯”åˆ†æ:\n")
        f.write("-" * 40 + "\n")
        comparison = results['comparison']
        
        if 'summary' in comparison:
            summary = comparison['summary']
            f.write(f"è¯„ä¼°ç±»åˆ«æ•°: {summary.get('categories_evaluated', 'N/A')}\n")
            f.write(f"å¯¹æ¯”æŒ‡æ ‡æ•°: {summary.get('total_metrics_compared', 'N/A')}\n")
            f.write(f"æ€»å¯¹æ¯”æ¬¡æ•°: {summary.get('total_comparisons', 'N/A')}\n")
            f.write("\n")
        
        # æ”¹è¿›åˆ†æ
        if 'improvement_analysis' in comparison:
            f.write("ğŸ“ˆ æ”¹è¿›åˆ†æ:\n")
            improvement = comparison['improvement_analysis']
            for metric, data in improvement.items():
                f.write(f"\n  {metric}:\n")
                f.write(f"    å¯¹æ¯”ç±»åˆ«æ•°: {data['categories_compared']}\n")
                f.write(f"    å®Œæ•´æ¨¡å‹æ›´ä¼˜: {data['full_better_count']}\n")
                f.write(f"    æ¶ˆèæ¨¡å‹æ›´ä¼˜: {data['ablation_better_count']}\n")
                f.write(f"    ç›¸ç­‰: {data['equal_count']}\n")
                f.write(f"    å¹³å‡æ”¹è¿›: {data['average_improvement']:.2f}%\n")
        
        # è¯¦ç»†å¯¹æ¯”
        if 'category_comparison' in comparison:
            f.write("\nğŸ“‹ å„ç±»åˆ«è¯¦ç»†å¯¹æ¯”:\n")
            for category, metrics in comparison['category_comparison'].items():
                f.write(f"\n  ğŸ·ï¸ {category}:\n")
                if 'error' in metrics:
                    f.write(f"    âŒ é”™è¯¯: {metrics['error']}\n")
                else:
                    for metric, data in metrics.items():
                        f.write(f"    {metric}:\n")
                        f.write(f"      å®Œæ•´æ¨¡å‹: {data['full_model']:.6f}\n")
                        f.write(f"      æ¶ˆèæ¨¡å‹: {data['ablation_model']:.6f}\n")
                        f.write(f"      æ”¹è¿›å¹…åº¦: {data['improvement_percent']:.2f}%\n")
                        f.write(f"      æ›´ä¼˜æ¨¡å‹: {data['better_model']}\n")


def write_model_results(f, model_results, indent=""):
    """å†™å…¥å•ä¸ªæ¨¡å‹çš„ç»“æœ"""
    if 'overall_stats' in model_results:
        stats = model_results['overall_stats']
        f.write(f"{indent}æ•´ä½“ç»Ÿè®¡:\n")
        f.write(f"{indent}  æ€»æ ·æœ¬æ•°: {stats.get('total_samples', 'N/A'):,}\n")
        f.write(f"{indent}  ç±»åˆ«æ•°é‡: {stats.get('total_categories', 'N/A')}\n")
        f.write(f"{indent}  æ¨¡å‹ç±»å‹: {stats.get('model_type', 'N/A')}\n")
    
    if 'evaluation_summary' in model_results:
        summary = model_results['evaluation_summary']
        f.write(f"{indent}è¯„ä¼°æ±‡æ€»:\n")
        f.write(f"{indent}  æ€»ç±»åˆ«æ•°: {summary.get('total_categories_evaluated', 'N/A')}\n")
        f.write(f"{indent}  æˆåŠŸè¯„ä¼°: {summary.get('successful_evaluations', 'N/A')}\n")
        f.write(f"{indent}  å¤±è´¥è¯„ä¼°: {summary.get('failed_evaluations', 'N/A')}\n")
    
    if 'category_results' in model_results:
        f.write(f"{indent}ç±»åˆ«ç»“æœ:\n")
        category_results = model_results['category_results']
        
        for category_name, category_result in category_results.items():
            f.write(f"{indent}  {category_name}:\n")
            
            if 'error' in category_result:
                f.write(f"{indent}    âŒ é”™è¯¯: {category_result['error']}\n")
            else:
                # åªæ˜¾ç¤ºä¸»è¦æ•°å€¼æŒ‡æ ‡
                main_metrics = {}
                for key, value in category_result.items():
                    if isinstance(value, (int, float)) and key.lower() in ['rmse', 'mae', 'auc', 'accuracy', 'precision', 'recall', 'f1']:
                        main_metrics[key] = value
                
                if main_metrics:
                    for metric, value in main_metrics.items():
                        f.write(f"{indent}    {metric}: {value:.6f}\n")
                else:
                    f.write(f"{indent}    æ— ä¸»è¦æ•°å€¼æŒ‡æ ‡\n")


def write_category_summary(f, results):
    """å†™å…¥æ™®é€šåˆ†ç±»åˆ«è¯„ä¼°æ±‡æ€»"""
        # æ•´ä½“ç»Ÿè®¡
    if 'overall_stats' in results:
            stats = results['overall_stats']
            f.write("ğŸ“ˆ æ•´ä½“ç»Ÿè®¡:\n")
            f.write("-" * 40 + "\n")
            f.write(f"æ€»æ ·æœ¬æ•°: {stats.get('total_samples', 'N/A'):,}\n")
            f.write(f"ç±»åˆ«æ•°é‡: {stats.get('total_categories', 'N/A')}\n")
            f.write("\n")
        
        # è¯„ä¼°æ±‡æ€»
    if 'evaluation_summary' in results:
            summary = results['evaluation_summary']
            f.write("ğŸ¯ è¯„ä¼°æ±‡æ€»:\n")
            f.write("-" * 40 + "\n")
            f.write(f"æ€»ç±»åˆ«æ•°: {summary.get('total_categories_evaluated', 'N/A')}\n")
            f.write(f"æˆåŠŸè¯„ä¼°: {summary.get('successful_evaluations', 'N/A')}\n")
            f.write(f"å¤±è´¥è¯„ä¼°: {summary.get('failed_evaluations', 'N/A')}\n")
            f.write("\n")
        
        # åˆ†ç±»åˆ«ç»“æœ
    if 'category_results' in results:
            f.write("ğŸ“‹ åˆ†ç±»åˆ«è¯„ä¼°ç»“æœ:\n")
            f.write("-" * 40 + "\n")
            
            category_results = results['category_results']
            
            # é¦–å…ˆæ˜¾ç¤ºæ•´ä½“ç»“æœ
            if 'Overall' in category_results:
                f.write("ğŸŒŸ æ•´ä½“è¯„ä¼°:\n")
                overall_result = category_results['Overall']
                if 'error' in overall_result:
                    f.write(f"   âŒ é”™è¯¯: {overall_result['error']}\n")
                else:
                    for key, value in overall_result.items():
                        if isinstance(value, (int, float)):
                            f.write(f"   {key}: {value:.6f}\n")
                        else:
                            f.write(f"   {key}: {value}\n")
                f.write("\n")
            
            # æ˜¾ç¤ºå„ç±»åˆ«ç»“æœ
            for category_name, category_result in category_results.items():
                if category_name == 'Overall':
                    continue
                    
                f.write(f"ğŸ·ï¸  {category_name}:\n")
                
                if 'error' in category_result:
                    f.write(f"   âŒ é”™è¯¯: {category_result['error']}\n")
                else:
                    # æ˜¾ç¤ºä¸»è¦æŒ‡æ ‡
                    for key, value in category_result.items():
                        if isinstance(value, (int, float)):
                            f.write(f"   {key}: {value:.6f}\n")
                        else:
                            f.write(f"   {key}: {value}\n")
                f.write("\n")
        
    
def print_console_summary(results, is_ablation):
    """åœ¨æ§åˆ¶å°æ‰“å°æ±‡æ€»ä¿¡æ¯"""
    print("\n" + "="*80)
    if is_ablation:
        print("ğŸ§ª æ¶ˆèå®éªŒè¯„ä¼°ç»“æœæ±‡æ€»")
        print("="*80)
        
        # æ˜¾ç¤ºä¸¤ç§æ¨¡å‹çš„å¯¹æ¯”
        if 'comparison' in results and 'summary' in results['comparison']:
            summary = results['comparison']['summary']
            print(f"ğŸ“Š å¯¹æ¯”åˆ†æ:")
            print(f"   è¯„ä¼°ç±»åˆ«æ•°: {summary.get('categories_evaluated', 'N/A')}")
            print(f"   å¯¹æ¯”æŒ‡æ ‡æ•°: {summary.get('total_metrics_compared', 'N/A')}")
            print(f"   æ€»å¯¹æ¯”æ¬¡æ•°: {summary.get('total_comparisons', 'N/A')}")
            print()
        
        # æ˜¾ç¤ºæ”¹è¿›æƒ…å†µ
        if 'comparison' in results and 'improvement_analysis' in results['comparison']:
            print("ğŸ“ˆ ä¸»è¦æŒ‡æ ‡æ”¹è¿›æƒ…å†µ:")
            improvement = results['comparison']['improvement_analysis']
            for metric, data in improvement.items():
                better_model = "å®Œæ•´æ¨¡å‹" if data['full_better_count'] > data['ablation_better_count'] else "æ¶ˆèæ¨¡å‹"
                print(f"   {metric}: {better_model}åœ¨{data['categories_compared']}ä¸ªç±»åˆ«ä¸­è¡¨ç°æ›´ä¼˜ (å¹³å‡æ”¹è¿›: {data['average_improvement']:.2f}%)")
            print()
        
    else:
        print("ğŸ“Š åˆ†ç±»åˆ«è¯„ä¼°ç»“æœæ±‡æ€»")
        print("="*80)
    
    # æ˜¾ç¤ºæ•´ä½“ç»Ÿè®¡
    if 'overall_stats' in results:
        stats = results['overall_stats']
        print(f"ğŸ“ˆ æ•´ä½“ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {stats.get('total_samples', 'N/A'):,}")
        print(f"   ç±»åˆ«æ•°é‡: {stats.get('total_categories', 'N/A')}")
        print()
    
    # æ˜¾ç¤ºè¯„ä¼°æ±‡æ€»
    if 'evaluation_summary' in results:
        summary = results['evaluation_summary']
        print(f"ğŸ¯ è¯„ä¼°æ±‡æ€»:")
        print(f"   æ€»ç±»åˆ«æ•°: {summary.get('total_categories_evaluated', 'N/A')}")
        print(f"   æˆåŠŸè¯„ä¼°: {summary.get('successful_evaluations', 'N/A')}")
        print(f"   å¤±è´¥è¯„ä¼°: {summary.get('failed_evaluations', 'N/A')}")
        print()
    
    # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡å¯¹æ¯”
    if 'category_results' in results:
        print("ğŸ“‹ å„ç±»åˆ«ä¸»è¦æŒ‡æ ‡:")
        print("-" * 60)
        
        category_results = results['category_results']
        
        for category_name, category_result in category_results.items():
            if 'error' not in category_result:
                # æŸ¥æ‰¾ä¸»è¦çš„æ•°å€¼æŒ‡æ ‡
                main_metrics = {}
                for key, value in category_result.items():
                    if isinstance(value, (int, float)) and key.lower() in ['rmse', 'mae', 'auc', 'accuracy', 'precision', 'recall', 'f1']:
                        main_metrics[key] = value
                
                print(f"ğŸ·ï¸  {category_name}:")
                if main_metrics:
                    for metric, value in main_metrics.items():
                        print(f"     {metric}: {value:.6f}")
                else:
                    print("     æ— æ•°å€¼æŒ‡æ ‡")
                print()
    
    print("="*80)


def parse_args():
    parser = argparse.ArgumentParser(description="CoRAæ¨¡å‹è¯„ä¼°è„šæœ¬")
    
    parser.add_argument(
        "--cfg-path", 
        default='train_configs/plora_pretrain_mf_ood.yaml',
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--checkpoint-path", 
        required=True,
        help="checkpointæ–‡ä»¶è·¯å¾„ (å¦‚: /root/autodl-tmp/checkpoints/[job_id]/checkpoint_best.pth)"
    )
    parser.add_argument(
        "--test-data-path",
        default="/root/autodl-tmp/dataset/amazon/test_ood2.pkl",
        help="æµ‹è¯•æ•°æ®è·¯å¾„"
    )
    parser.add_argument(
        "--output-dir",
        default="/root/autodl-tmp/eval_results",
        help="è¯„ä¼°ç»“æœä¿å­˜ç›®å½•"
    )
    parser.add_argument(
        "--device",
        default="auto",
        choices=["auto", "cpu", "cuda"],
        help="è®¡ç®—è®¾å¤‡"
    )
    parser.add_argument(
        "--ablation",
        action="store_true",
        help="è¿è¡Œæ¶ˆèå®éªŒ (å¯¹æ¯”ä½¿ç”¨å’Œä¸ä½¿ç”¨é¢„è®­ç»ƒCFæ¨¡å‹çš„æ•ˆæœ)"
    )
    parser.add_argument(
        "--disable-cf",
        action="store_true",
        help="ç¦ç”¨CFæ¨¡å‹ç»„ä»¶ (ä»…åœ¨éæ¶ˆèå®éªŒæ¨¡å¼ä¸‹æœ‰æ•ˆ)"
    )
    parser.add_argument(
        "--options",
        nargs="+",
        help="override some settings in the used config, the key-value pair "
        "in xxx=yyy format will be merged into config file (deprecate), "
        "change to --cfg-options instead.",
    )
    
    return parser.parse_args()


def setup_seeds(seed=42):
    """è®¾ç½®éšæœºç§å­"""
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    cudnn.benchmark = False
    cudnn.deterministic = True


def main():
    # è§£æå‚æ•°
    args = parse_args()
    
    # ç¡®å®šè¯„ä¼°æ¨¡å¼
    if args.ablation:
        eval_mode = "æ¶ˆèå®éªŒæ¨¡å¼"
        print("ğŸ§ª CoRAæ¨¡å‹æ¶ˆèå®éªŒè¯„ä¼°è„šæœ¬å¯åŠ¨")
        print("å°†å¯¹æ¯”ä»¥ä¸‹ä¸¤ç§é…ç½®:")
        print("  1. å®Œæ•´æ¨¡å‹ (åŒ…å«é¢„è®­ç»ƒCFæ¨¡å‹)")
        print("  2. æ¶ˆèæ¨¡å‹ (ä¸ä½¿ç”¨é¢„è®­ç»ƒCFæ¨¡å‹)")
    else:
        eval_mode = "å•æ¨¡å‹è¯„ä¼°æ¨¡å¼"
        cf_status = "(ç¦ç”¨CFæ¨¡å‹)" if args.disable_cf else "(å®Œæ•´æ¨¡å‹)"
        print(f"ğŸš€ CoRAæ¨¡å‹è¯„ä¼°è„šæœ¬å¯åŠ¨ {cf_status}")
    
    print(f"é…ç½®æ–‡ä»¶: {args.cfg_path}")
    print(f"Checkpoint: {args.checkpoint_path}")
    print(f"æµ‹è¯•æ•°æ®: {args.test_data_path}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"è¯„ä¼°æ¨¡å¼: {eval_mode}")
    
    # è®¾ç½®éšæœºç§å­
    setup_seeds()
    
    # è®¾ç½®æ—¥å¿—
    setup_logger()
    
    try:
        # 1. åŠ è½½é…ç½®
        print("\nğŸ“‹ 1. åŠ è½½é…ç½®æ–‡ä»¶...")
        cfg = Config(args)
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        cfg.run_cfg.evaluate = True
        cfg.run_cfg.distributed = False
        
        # ç¡®ä¿æµ‹è¯•åˆ†å‰²è¢«è®¾ç½®
        cfg.run_cfg.test_splits = ["test"]
        cfg.run_cfg.train_splits = []
        cfg.run_cfg.valid_splits = []
        
        print("âœ… é…ç½®åŠ è½½å®Œæˆ")
        
        if args.ablation:
            # æ¶ˆèå®éªŒæ¨¡å¼
            print("\nğŸ§ª 2. è¿è¡Œæ¶ˆèå®éªŒ...")
            results = run_ablation_evaluation(args.checkpoint_path, cfg, args.test_data_path, args.output_dir)
            print("âœ… æ¶ˆèå®éªŒå®Œæˆ")
            
            # ä¿å­˜æ¶ˆèå®éªŒç»“æœ
            print("\nğŸ’¾ 3. ä¿å­˜æ¶ˆèå®éªŒç»“æœ...")
            results_file, summary_file = save_results(results, args.output_dir, args.checkpoint_path, is_ablation=True)
            print("âœ… ç»“æœä¿å­˜å®Œæˆ")
            
            print(f"\nğŸ‰ æ¶ˆèå®éªŒæˆåŠŸå®Œæˆï¼")
            print(f"ğŸ“ è¯¦ç»†ç»“æœæ–‡ä»¶: {results_file}")
            print(f"ğŸ“„ æ±‡æ€»æŠ¥å‘Šæ–‡ä»¶: {summary_file}")
            
        else:
            # å•æ¨¡å‹è¯„ä¼°æ¨¡å¼
            print("\nğŸ¤– 2. åŠ è½½æ¨¡å‹å’Œcheckpoint...")
            model, task, datasets = load_checkpoint_and_model(args.checkpoint_path, cfg, disable_cf_model=args.disable_cf)
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # 3. è¿è¡Œè¯„ä¼°
        print("\nğŸ”¬ 3. è¿è¡Œè¯„ä¼°...")
        model_type = "ablation" if args.disable_cf else "full"
        results = run_single_model_evaluation(model, task, datasets, cfg, args.test_data_path, model_type)
        print("âœ… è¯„ä¼°å®Œæˆ")
        
        # 4. ä¿å­˜ç»“æœ
        print("\nğŸ’¾ 4. ä¿å­˜è¯„ä¼°ç»“æœ...")
        results_file, summary_file = save_results(results, args.output_dir, args.checkpoint_path, is_ablation=False)
        print("âœ… ç»“æœä¿å­˜å®Œæˆ")
        
        print(f"\nğŸ‰ åˆ†ç±»åˆ«è¯„ä¼°æˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“ è¯¦ç»†ç»“æœæ–‡ä»¶: {results_file}")
        print(f"ğŸ“„ æ±‡æ€»æŠ¥å‘Šæ–‡ä»¶: {summary_file}")
        
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
